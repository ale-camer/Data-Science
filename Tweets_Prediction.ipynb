{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ale-camer/Data-Science/blob/Finance/Tweets_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is a code in which we compare the prediction capabilities of Machine Learning (Decision Tree) and Deep Learning (LSTM) algorithms for text data. More precisely, we will predict whether or not tweets were sent (inbound) to a company. For this purpose we use a dataset from a [Kaggle competition](https://www.kaggle.com/datasets/thoughtvector/customer-support-on-twitter?select=sample.csv). \n",
        "\n",
        "If you want to download the dataset directely to your Colab see the following video in [YouTube](https://www.youtube.com/watch?v=T8xEQI8XXGs&ab_channel=AI-SPECIALS). "
      ],
      "metadata": {
        "id": "Y3qEY522jQsa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9Eok9qjN-FV"
      },
      "source": [
        "## Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJj--wikLU6E"
      },
      "outputs": [],
      "source": [
        "!pip install unidecode --quiet\n",
        "import re\n",
        "import nltk\n",
        "import sklearn\n",
        "import warnings\n",
        "import unidecode\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import en_core_web_sm\n",
        "from nltk.corpus import stopwords\n",
        "from prettytable import PrettyTable\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize.casual import TweetTokenizer\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential#, Model\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM, SpatialDropout1D\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJFD8SodOCLu"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3873zgD_CzW6",
        "outputId": "0afb2e10-5fad-4808-8945-7079ec03fd8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  inbound\n",
            "0  @115712 I understand. I would like to assist y...        0\n",
            "1      @sprintcare and how do you propose we do that        1\n",
            "2  @sprintcare I have sent several private messag...        1\n",
            "3  @115712 Please send us a Private Message so th...        0\n",
            "4                                 @sprintcare I did.        1 \n",
            "\n",
            " Distribution of labels to predict: \n",
            "1    0.55\n",
            "0    0.45\n",
            "Name: inbound, dtype: float64 \n",
            "\n",
            " Number of rows: 10,000\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('/content/twcs.csv') # reading data\n",
        "data, nRows = data[['text','inbound']], 10000 # keeping only necessary columns and 10,000 rows in order to make it quicker. The dataset contains millions of rows.\n",
        "data['inbound'] = OrdinalEncoder().fit_transform(data[['inbound']]) # discretization of target data\n",
        "data['inbound'] = data['inbound'].astype('int')\n",
        "data = data.iloc[:nRows,:]\n",
        "print(data.head(),f\"\\n\\n Distribution of labels to predict: \\n{round(data['inbound'].value_counts(normalize=True),2)}\",\n",
        "      f\"\\n\\n Number of rows: {format(data.shape[0],',d')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUJkukyzOAwU"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwuYGE8MODFs"
      },
      "outputs": [],
      "source": [
        "class Normalizer(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
        "\n",
        "    \"\"\"\n",
        "    This function normalize tweet data by:\n",
        "      - deleting stopwords, words with less than certain length and URLs, \n",
        "      - replacing diacritical marks and capital letter,\n",
        "      - doing lemmatization or stemming \n",
        "      - and tokenizing data.  \n",
        "\n",
        "    Returns a transformed text or a sequence of tokens (text2seq).\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, language='english', minWordLen=2, lemmatize=True, stem=False, stripSpeChar=True, stripUrls=True, stripStopwords=True, \n",
        "                 text2Seq=False):\n",
        "\n",
        "        nltk.download('stopwords', quiet=True) # download stopwords\n",
        "        self.stopwords = set(stopwords.words(language)) # select the language of the stopwords\n",
        "\n",
        "        self.numAndChar = r'[^a-zA-Z0-9\\s]' # RegEx for letters and numbers\n",
        "        self.urlRegex = re.compile('http\\S+') # RegEx to delete URLs\n",
        "\n",
        "        self.tweetTokenizer = TweetTokenizer() # tokenizer instantiation\n",
        "        self.parser = en_core_web_sm.load() # parser instantiation\n",
        "\n",
        "        self.minWordLen = minWordLen # minimum length of the words\n",
        "        self.stripUrls = stripUrls # boolean to delete URLs\n",
        "        self.stripStopwords = stripStopwords # boolean to delete stopwords \n",
        "        self._text2Seq = text2Seq\n",
        "        self.stripSpeChar = stripSpeChar\n",
        "\n",
        "        if stem: # normalization control\n",
        "            self.stemmer = nltk.stem.SnowballStemmer(language=language) # steammer instantiation\n",
        "        else:\n",
        "            self.stemmer = False\n",
        "        \n",
        "        if lemmatize:\n",
        "            self.lemmatizer = lambda word : \" \".join([token.lemma_ for token in self.parser(word)]) # lemmatizer instantiation\n",
        "        else:\n",
        "            self.lemmatizer = False\n",
        "\n",
        "    def textProcessor(self, text):\n",
        "\n",
        "        if (self.stripSpeChar):\n",
        "          tokens = re.sub(self.numAndChar, '', text.lower()) # replace capital letters and delete special characters\n",
        "  \n",
        "        tokens = self.tweetTokenizer.tokenize(tokens) # tweet tokenizer\n",
        "\n",
        "        if (self.stripUrls): # delete URLs\n",
        "            tokens = [token for token in tokens if not re.match(self.urlRegex, token)]\n",
        "\n",
        "        tokens = [token for token in tokens if len(token) > self.minWordLen] # delete words with less than 'x' letters\n",
        "        tokens = [unidecode.unidecode(token) for token in tokens] # replace diacritical marks\n",
        "\n",
        "        if (self.stripStopwords): # delete stopwords\n",
        "            tokens = [token for token in tokens if token not in self.stopwords]\n",
        "\n",
        "        if self.lemmatizer: # lemmatization\n",
        "            tokens = [self.lemmatizer(token) for token in tokens]\n",
        "        if self.stemmer: # stemming\n",
        "            tokens = [self.stemmer.stem(token) for token in tokens]\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        if self._text2Seq:\n",
        "            for doc in X:\n",
        "                yield self.textProcessor(text=doc)\n",
        "        else:\n",
        "            for doc in X:\n",
        "                yield ' '.join(self.textProcessor(text=doc))\n",
        "\n",
        "class PadSeqTransf(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
        "\n",
        "    \"\"\"\n",
        "    This function transform a list of sequences of number of samples into a 2D numpy array of number of samples and number of timesteps.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_len=None, padding=\"pre\"):\n",
        "\n",
        "        self.max_len = max_len\n",
        "        self.padding = padding\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return pad_sequences(list(X), maxlen=self.max_len, padding=self.padding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW75oVqZMyj2"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgumwMX1IQro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5a341c6-2f7b-4f01-dbc8-2f96863ff013"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------------------------+\n",
            "|                              Model Comparison                             |\n",
            "+---------------------------+------------------------+----------------------+\n",
            "| Machine Learning Accuracy | Deep Learning Accuracy | Which one is better? |\n",
            "+---------------------------+------------------------+----------------------+\n",
            "|           60.15%          |         96.5%          |    Deep Learning     |\n",
            "|           61.15%          |         96.6%          |    Deep Learning     |\n",
            "|           58.2%           |         96.35%         |    Deep Learning     |\n",
            "|           58.65%          |         96.4%          |    Deep Learning     |\n",
            "|           58.15%          |         96.1%          |    Deep Learning     |\n",
            "|           63.8%           |         96.05%         |    Deep Learning     |\n",
            "|           65.15%          |         96.05%         |    Deep Learning     |\n",
            "|           60.85%          |         95.95%         |    Deep Learning     |\n",
            "|           60.1%           |         95.95%         |    Deep Learning     |\n",
            "|           60.3%           |         95.85%         |    Deep Learning     |\n",
            "+---------------------------+------------------------+----------------------+\n"
          ]
        }
      ],
      "source": [
        "# Constants\n",
        "maxSeqLen, emdeddingsize, testSize, nClasses, table, nWords, activation, outputName = 50, 100, 0.2, data['inbound'].nunique(), PrettyTable(), 10000, \"softmax\", \"output\"\n",
        "loss, optimizer, metric, mask_zero, dropout, batchSize, epochs = 'sparse_categorical_crossentropy', 'adam', \"accuracy\", True, 0.2, 124, 3\n",
        "\n",
        "# Machine Learning Pipeline and Data Split\n",
        "MLnormalizer, MLvectorizer, MLfeaturizer, MLestimator = Normalizer(), TfidfVectorizer(), LatentDirichletAllocation(), DecisionTreeClassifier()\n",
        "MLpipeline = Pipeline(steps=[('normalizer', MLnormalizer),('vectorizer', MLvectorizer),('featurizer', MLfeaturizer),('estimator', MLestimator)])\n",
        "MLX_train, MLX_test, MLy_train, MLy_test = train_test_split(data['text'], data['inbound'], test_size=testSize, stratify=data['inbound'])\n",
        "\n",
        "# Deep Learning Pipeline and Data Split\n",
        "DLnormalizer = Normalizer(text2Seq=True)\n",
        "DLnormalizer, DLtokenizer = list(DLnormalizer.transform(data[\"text\"])), Tokenizer(num_words=nWords)\n",
        "DLtokenizer.fit_on_texts(DLnormalizer)\n",
        "inputTweets = DLtokenizer.texts_to_sequences(DLnormalizer)\n",
        "vocabSize, padder = DLtokenizer.num_words + 1, PadSeqTransf(max_len=maxSeqLen)\n",
        "inputTweetsPadded = padder.transform(inputTweets)\n",
        "inputTweets =  np.array(inputTweetsPadded).astype('int32')\n",
        "\n",
        "DLtrain, DLtest = train_test_split(range(len(inputTweets)),test_size=testSize,stratify=data['inbound'])\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder = label_encoder.fit(data[\"inbound\"])\n",
        "nClasses2 = len(label_encoder.classes_)\n",
        "DLX_train, DLy_train, DLX_test, DLy_test = np.array(inputTweets[DLtrain]).astype('int32'), np.array(label_encoder.transform(data[\"inbound\"][DLtrain])), \\\n",
        "  np.array(inputTweets[DLtest]).astype('int32'), np.array(label_encoder.transform(data[\"inbound\"][DLtest]))\n",
        "DLmodel = Sequential([Embedding(input_dim=vocabSize,input_length=maxSeqLen,output_dim=emdeddingsize,mask_zero=mask_zero),SpatialDropout1D(dropout),\n",
        "                      LSTM(emdeddingsize),Dense(nClasses2, activation=activation, name=outputName)])\n",
        "DLmodel.compile(loss=loss,optimizer=optimizer,metrics=[metric])\n",
        "\n",
        "# Prediction and Evaluation\n",
        "table.field_names = ['Machine Learning Accuracy', 'Deep Learning Accuracy', 'Which one is better?']\n",
        "table.title = 'Model Comparison'\n",
        "\n",
        "for _ in range(10):\n",
        "\n",
        "  MLmodel = MLpipeline.fit(X=MLX_train, y=MLy_train)\n",
        "  MLpreds = MLmodel.predict(MLX_test)\n",
        "  MLacc = round(accuracy_score(MLy_test, MLpreds)*100,2)\n",
        "\n",
        "  DLmodelFit = DLmodel.fit(DLX_train,DLy_train,batch_size=batchSize,epochs=epochs,validation_data=(DLX_test, DLy_test),verbose=0)\n",
        "  DLpreds = DLmodel.predict(DLX_test).argmax(axis=-1)\n",
        "  DLacc = round(accuracy_score(DLy_test, DLpreds)*100,2)\n",
        "  \n",
        "  if MLacc > DLacc:\n",
        "    statement = \"Machine Learning\"\n",
        "  else:\n",
        "    statement = \"Deep Learning\"\n",
        "\n",
        "  table.add_row([f\"{MLacc}%\",f\"{DLacc}%\",statement])\n",
        "print(table)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Tweets Prediction.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}